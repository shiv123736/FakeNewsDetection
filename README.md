# Fake News Detection Using NLP and BERT

A **fully offline-capable**, end-to-end project to classify news as **Fake** or **Real** using BERT/DistilBERT,
with a **Streamlit** UI and **LIME** explainability.

## ğŸ”§ Quick Start

### 1) Set Up Environment
```bash
# Windows
pip install -r requirements-windows.txt

# Other platforms
pip install -r requirements.txt

# Download NLTK resources
python -m nltk.downloader punkt stopwords wordnet
```

### 2) Activate Virtual Environment (Windows PowerShell)
```powershell
.\.venv\Scripts\Activate.ps1
```

### 3) Download Dataset
```bash
# Run the data download script
python src/data_download.py
```
This will help you download the required datasets (`Fake.csv` and `True.csv`) to the `data` directory.

### 4) Preprocess & Split Data
```bash
python src/preprocessing.py --fake_path data/Fake.csv --real_path data/True.csv --out_dir data --val_size 0.1 --test_size 0.1
```
This creates `data/train.csv`, `data/val.csv`, and `data/test.csv`.

### 5) Train the Model (DistilBERT by default)
```bash
# Enhanced automatic GPU detection and optimization
python src/train_model_auto.py --train_path data/train.csv --val_path data/val.csv

# Legacy training script (original version)
python src/train_model.py --train_path data/train.csv --val_path data/val.csv --model_name distilbert-base-uncased --out_dir model
```

For better performance, install accelerate:
```bash
pip install accelerate -U
```

To train a **multilingual** model:
```bash
# Train multilingual model with a different output directory
python src/train_model_auto.py --model_name bert-base-multilingual-cased --train_path data/train.csv --val_path data/val.csv --out_dir model_multilingual

# Or using the legacy script
python src/train_model.py --model_name bert-base-multilingual-cased
```

Advanced GPU optimization options:
```bash
# Control GPU memory usage (0.0-1.0)
python src/train_model_auto.py --gpu_memory_fraction 0.8

# Custom batch size
python src/train_model_auto.py --batch_size 32

# Disable mixed precision (for compatibility issues)
python src/train_model_auto.py --no_fp16
```

### 6) Test / Predict
```bash
python src/predict.py --text "NASA confirms 6 days of darkness in December 2025 due to solar storm."
python src/predict.py --text "Local school wins regional basketball championship for third year in a row."
python src/predict.py --input_file data/test.csv --output_file predictions.csv

# Test on a batch of examples from a file
python src/predict.py --input_file data/test.csv --output_file predictions.csv
```
## ğŸ§  Explainability (XAI)
The project provides LIME-based explanations that highlight influential words for a given prediction. This helps understand why the model classifies news as fake or real.
```bash
python src/explain.py --text "Your test news headline here"
python src/explain.py --text "NASA confirms 6 days of darkness in December 2025 due to solar storm."
```

### 7) Run the Streamlit App
```bash
streamlit run app/streamlit_app.py
```
Open the local URL shown (usually http://localhost:8501).

---

## ğŸ“ Project Structure
```
FakeNewsDetection_Starter_Kit/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ streamlit_app.py           # Streamlit web interface
â”œâ”€â”€ data/                          # Dataset directory (created by data_download.py)
â”‚   â”œâ”€â”€ Fake.csv                   # Fake news dataset (downloaded)
â”‚   â”œâ”€â”€ True.csv                   # Real news dataset (downloaded)
â”‚   â”œâ”€â”€ train.csv                  # Generated by preprocessing.py
â”‚   â”œâ”€â”€ val.csv                    # Generated by preprocessing.py
â”‚   â””â”€â”€ test.csv                   # Generated by preprocessing.py
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Train_BERT.ipynb           # Jupyter notebook for model training
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_download.py           # Script to download dataset files
â”‚   â”œâ”€â”€ preprocessing.py           # Data cleaning and preparation
â”‚   â”œâ”€â”€ train_model.py             # Original model training script
â”‚   â”œâ”€â”€ train_model_auto.py        # Enhanced training with GPU optimization
â”‚   â”œâ”€â”€ predict.py                 # Make predictions on new data
â”‚   â”œâ”€â”€ explain.py                 # Model explanation using LIME
â”‚   â””â”€â”€ test_env.py                # Environment testing
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ requirements-windows.txt       # Windows-specific dependencies
â””â”€â”€ README.md                      # This file
```

## ğŸ§ª Evaluation Metrics
The model reports the following metrics:
- **Accuracy**: Overall correct predictions
- **Precision**: True positives / (True positives + False positives)
- **Recall**: True positives / (True positives + False negatives)
- **F1 Score**: Harmonic mean of precision and recall

A confusion matrix is also printed after evaluation.

## ğŸŒ Multilingual Support
This project can be extended to support multiple languages:
- Use `bert-base-multilingual-cased` as the base model
- Add multilingual datasets and rerun preprocessing/training
- Compare performance across languages

## ğŸ”’ Offline Capability
Once you've downloaded the required files:
- No internet connection required for inference
- Model weights are cached locally in `~/.cache/huggingface`
- All processing happens on your local machine

## ğŸ” Troubleshooting
- If you encounter CUDA/GPU issues:
  - Run `python src/test_env.py` to check GPU availability
  - Use `train_model_auto.py` for automatic GPU optimization
  - Add `--gpu_memory_fraction 0.7` to reduce memory usage
  - Try `--no_fp16` flag to disable mixed precision
- For memory issues, reduce `--batch_size` (default: auto-configured)
- If training is slow, ensure your GPU is being utilized (see GPU monitoring output)

## ğŸ“œ License
This project is for academic and research purposes.
